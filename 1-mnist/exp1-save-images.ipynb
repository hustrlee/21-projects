{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c40cd7",
   "metadata": {},
   "source": [
    "# 实验：将 MNIST 数据集保存为图片\n",
    "----\n",
    "\n",
    "**实验目的：**更深入地理解 `tf.data.Datasets` 数据结构，方便后面进行数据操作。\n",
    "\n",
    "**实验内容：**将 MNIST 数据集中的前 20 张图片保存到 `./mnist_data/raw/` 下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df746d33",
   "metadata": {},
   "source": [
    "## 研究 `tensorflow_datasets` 和 `tf.data.Dataset` 的基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0e0996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>, 'train': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "mnist = tfds.load(name=\"mnist\", data_dir=\"./mnist_data/\")\n",
    "print(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98f542",
   "metadata": {},
   "source": [
    "> **MNIST 数据集包括：`train` 和 `test` 两个部分，用 `dict` 的数据结构进行组织。可以通过字典操作来分别访问这两个子集。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6eec9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = mnist[\"test\"]\n",
    "mnist_train = mnist[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0319e3",
   "metadata": {},
   "source": [
    "> **或者，在 `load` 时，分别读取这两个子集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93d37a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = tfds.load(name=\"mnist\", split=\"test\", data_dir=\"./mnist_data/\")\n",
    "mnist_train = tfds.load(name=\"mnist\", split=\"train\", data_dir=\"./mnist_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d9429",
   "metadata": {},
   "source": [
    "> **`mnist_test` 和 `mnist_train` 都是 `tf.data.Dataset` 类的实例。**\n",
    ">\n",
    "> **注意：是 `tf.data.Dataset` 的实例，而不是 `list<tf.data.Dataset>`。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf3f4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(mnist_test, tf.data.Dataset)\n",
    "assert isinstance(mnist_train, tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffdcabc",
   "metadata": {},
   "source": [
    "> - **`tf.data.Dataset` 用于管理数据集的所有数据。**\n",
    ">\n",
    "> - **`tf.data.Dataset` 是可迭代的，每次迭代的结果是数据集中的一个数据。**\n",
    "> \n",
    "> - **`tf.data.Dataset` 不支持“下标（subscriptable）”操作，例如：`mnist_test[0]` 是不允许的。**\n",
    ">\n",
    "> - **可以使用 `for` 来进行遍历 `tf.data.Dataset` 中的数据。**\n",
    ">\n",
    "> - **可以使用 `take()` 类方法，来生成 `tf.data.Dataset` 实例的一个子集（仍然是一个 `tf.data.Dataset` 实例）**\n",
    ">\n",
    "> - **帮助生成子集的类方法，还包括：`skip()` 和 `range()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d07976de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'label'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 20:49:44.371885: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# 获取 mnist_train 中的第一个数据的数据结构\n",
    "for el in mnist_train.take(1):\n",
    "    print(el.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce730322",
   "metadata": {},
   "source": [
    "> **数据集中的数据是一个 `dict`。在 MNIST 数据集中，每个数据包括两个字段：`image` 和 `label`。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c3be8d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 20:49:44.411593: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# 获取 mnist_test 中前10个数据的 label\n",
    "for el in mnist_test.take(10):\n",
    "    print(el[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f50c30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 20:49:44.453961: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# 获取 mnist_test 中第三个数据的 label\n",
    "for el in mnist_test.skip(2).take(1):\n",
    "    print(el[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85abdc5",
   "metadata": {},
   "source": [
    "> **可以将 `tf.data.Datasets` 转换成 `list` 来访问数据。**\n",
    ">\n",
    "> **注意：对于大数据集转换成 `list` 非常耗时，不如直接对 `tf.data.Dataset` 进行操作。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49e87dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 获取 mnist_test 中前10个数据的 label\n",
    "for el in list(mnist_test)[:10]:\n",
    "    print(el[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76f968",
   "metadata": {},
   "source": [
    "## 保存 `mnist_train` 中的前 20 张图片 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65ccab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 20:50:07.163311: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# 设置科学上网\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:1081\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:1081\"\n",
    "\n",
    "# 读取 MNIST train 数据集，如果不存在会事先下载\n",
    "mnist_train = tfds.load(name=\"mnist\", split=\"train\", data_dir=\"./mnist_data/\")\n",
    "\n",
    "# 把原始图片保存在 mnist_data/raw/ 目录下\n",
    "# 如果没有这个文件夹，则创建它\n",
    "save_dir = \"mnist_data/raw/\"\n",
    "if os.path.exists(save_dir) is False:\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# 保存前 20 张图片\n",
    "i = 0\n",
    "for el in mnist_train.take(20):\n",
    "    # tensorflow 中处理的都是张量（Tensor）。为了保存图片，需将其转换成 array\n",
    "    # MNIST 中图片的 shape = (28, 28, 1)，表示是一张 28x28 的灰度图片\n",
    "    image_array = el[\"image\"].numpy()\n",
    "    \n",
    "    # 图像文件的文件名格式为：\n",
    "    # mnist_train_0.jpg, mnist_train_1.jpg, ..., mnist_train_19.jpg\n",
    "    filename = save_dir + \"mnist_train_%d.jpg\" % i\n",
    "    \n",
    "    # 将 image_array 保存为图片\n",
    "    imageio.imsave(filename, image_array)\n",
    "    \n",
    "    # 更新文件索引\n",
    "    i += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
